{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["# **Project:** Home Credit Default Risk"]},{"cell_type":"markdown","metadata":{},"source":["## **Team Members:**\n","### Hari Kiran Gannamani, chaitanya chirumamilla, Shashi Preetham Podupuganti, Muhammad Naveed"]},{"cell_type":"markdown","metadata":{},"source":["### **Project Overview**\n","\n","**Context:** Many individuals struggle to secure loans due to insufficient credit histories, facing exploitation by predatory lenders.\n","\n","**Home Credit's Mission:** Aimed at enhancing financial inclusion, Home Credit utilizes alternative data (like telecom and transaction records) to evaluate loan repayment abilities, providing a safer borrowing experience for the underbanked.\n","\n","**Kaggle Challenge:** Home Credit seeks innovative solutions from Kagglers to improve their data's predictive accuracy, ensuring loans are accessible to those who can repay and are structured to support borrowers' financial success."]},{"cell_type":"markdown","metadata":{},"source":["### **Data Overview**\n","\n","**Source and Purpose:** The dataset is provided by Home Credit, a company focusing on lending to individuals with limited access to traditional banking services. The objective is to predict the likelihood of a client repaying a loan, addressing a key business challenge. This competition on Kaggle encourages the machine learning community to develop models that can assist in this prediction task.\n","\n","**Data Composition:** The dataset encompasses seven distinct sources, all in CSV format, detailing various aspects of loan applications and client credit history:\n","\n","1. **application_train/application_test:** These files constitute the primary dataset, containing records of loan applications at Home Credit. Each row represents a unique loan application, identified by `SK_ID_CURR`. The training data includes a `TARGET` feature indicating loan repayment status (0: repaid, 1: not repaid).\n","\n","2. **bureau:** This dataset provides information on the client's previous credits from other financial institutions, with each row representing a distinct previous credit. A single loan in the application data may be associated with multiple entries here.\n","\n","3. **bureau_balance:** Offers monthly details on previous credits listed in the bureau data. Each row corresponds to a month of credit, with multiple rows per credit reflecting its duration.\n","\n","4. **previous_application:** Contains data on prior loan applications at Home Credit by clients with current loans in the application data. Each row represents a previous application, identified by `SK_ID_PREV`, with the potential for multiple entries per current loan.\n","\n","5. **POS_CASH_BALANCE:** Presents monthly information on past point of sale or cash loans with Home Credit. Each entry covers a month of loan, with multiple entries per loan.\n","\n","6. **credit_card_balance:** Monthly data on previous credit cards held with Home Credit. Similar to other datasets, each row accounts for a month's balance, with multiple rows per credit card.\n","\n","7. **installments_payments:** Tracks payment history for past loans at Home Credit, including both made and missed payments. Each payment instance is recorded in a separate row.\n","\n","This structured and multifaceted dataset offers a comprehensive view of clients' financial behaviors and credit histories, aiming to enable precise loan repayment predictions."]},{"cell_type":"markdown","metadata":{},"source":["## Imports\n","\n","- We are using basic imports required for data science"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-04T04:26:08.115441Z","iopub.status.busy":"2024-04-04T04:26:08.114950Z","iopub.status.idle":"2024-04-04T04:26:08.127199Z","shell.execute_reply":"2024-04-04T04:26:08.125935Z","shell.execute_reply.started":"2024-04-04T04:26:08.115405Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'matplotlib'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Data Visualization Libraries\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m  \u001b[38;5;66;03m# For creating static, animated, and interactive visualizations\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m  \u001b[38;5;66;03m# For making attractive and informative statistical graphics\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Preprocessing Tools\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"]}],"source":["# Essential Libraries for Data Manipulation\n","import numpy as np  # For numerical operations\n","import pandas as pd  # For data manipulation and analysis\n","\n","# File System Management\n","import os  # For interacting with the operating system\n","\n","# Warnings Management\n","import warnings  # To suppress warnings which can clutter the notebook\n","warnings.filterwarnings('ignore')\n","\n","# Data Visualization Libraries\n","import matplotlib.pyplot as plt  # For creating static, animated, and interactive visualizations\n","import seaborn as sns  # For making attractive and informative statistical graphics\n","\n","# Preprocessing Tools\n","from sklearn.preprocessing import LabelEncoder  # For encoding labels with value between 0 and n_classes-1\n","\n","# Set matplotlib inline for Jupyter notebook (use only in a Jupyter notebook)\n","%matplotlib inline\n","\n","# Optionally, set a style for seaborn to make plots more appealing\n","sns.set(style=\"whitegrid\")  # This is optional and can be modified based on preference"]},{"cell_type":"markdown","metadata":{},"source":["### Load Training data"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-04T04:26:08.129987Z","iopub.status.busy":"2024-04-04T04:26:08.129512Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m app_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProjet+Mise+en+prod+-+home-credit-default-risk/application_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(app_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m app_train\u001b[38;5;241m.\u001b[39mhead()\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["app_train = pd.read_csv('Projet+Mise+en+prod+-+home-credit-default-risk/application_train.csv')\n","print(app_train.shape)\n","app_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["The training data has 307511 rows i.e. each one a separate loan and 122 features/columns including the TARGET (the label we want to predict)."]},{"cell_type":"markdown","metadata":{},"source":["### Load Testing data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["app_test = pd.read_csv('../input/home-credit-default-risk/application_test.csv')\n","print(app_test.shape)\n","app_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["The testing data has 48744 rows i.e. each one a separate loan and 121 features/columns which does not include the TARGET column. Testing dataset is considerably smaller than train data."]},{"cell_type":"markdown","metadata":{},"source":["### **Exploratory Data Analysis (EDA) Overview**\n","\n","Exploratory Data Analysis (EDA) plays a crucial role in understanding the underlying patterns, anomalies, relationships, and trends within a dataset. It's a foundational step in the data science process that precedes model building. EDA involves a mix of visualization and statistical analysis to uncover insights from data. It's an iterative and exploratory process, starting from a broad perspective and progressively focusing on specific aspects as interesting patterns emerge.\n","\n","**Case Study: Analyzing the TARGET Column**\n","\n","One of the initial steps in EDA, especially for a classification problem like the Home Credit Default Risk, is to examine the distribution of the target variable. In this case, the TARGET column indicates whether a loan was repaid (0) or not (1). Let's perform a basic analysis of this column to understand the class distribution:"]},{"cell_type":"markdown","metadata":{},"source":["## Examining `TARGET` column distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Examining the distribution of the 'TARGET' column\n","target_distribution = app_train['TARGET'].value_counts()\n","print(target_distribution)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Visualizing the distribution of the 'TARGET' column\n","app_train['TARGET'].astype(int).plot.hist()\n","plt.title('Distribution of the TARGET Variable')\n","plt.xlabel('TARGET Value')\n","plt.ylabel('Frequency')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Insights:**\n","\n","From the output, we observe that there are 282,686 instances where the loan was repaid (TARGET=0) and 24,825 instances where the loan was not repaid (TARGET=1).\n","This indicates a significant imbalance in the dataset: 91.93% of the loans were repaid on time, while only 8.07% were not. Such an imbalance can influence the performance of machine learning models, as they might become biased towards predicting the majority class."]},{"cell_type":"markdown","metadata":{},"source":["### Analyzing Missing Values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# Function to calculate missing values by column in a DataFrame\n","def missing_values_table(df):\n","    # Calculate total missing values\n","    mis_val = df.isnull().sum()\n","    \n","    # Calculate percentage of missing values\n","    mis_val_pct = 100 * df.isnull().sum() / len(df)\n","    \n","    # Make a table with the results\n","    mis_val_tab = pd.concat([mis_val, mis_val_pct], axis=1)\n","    \n","    # Rename the columns\n","    mis_val_tab_col_renamed = mis_val_tab.rename(\n","        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n","    \n","    # Sort the table by percentage of missing descending, after excluding columns with no missing values\n","    mis_val_tab_col_renamed = mis_val_tab_col_renamed[\n","        mis_val_tab_col_renamed.iloc[:,1] != 0].sort_values(\n","        '% of Total Values', ascending=False).round(1)\n","    \n","    # Print some summary information\n","    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n","           \"There are \" + str(mis_val_tab_col_renamed.shape[0]) + \n","           \" columns that have missing values.\")\n","    \n","    # Return the dataframe with missing information\n","    return mis_val_tab_col_renamed\n","# Missing values statistics\n","missing_values = missing_values_table(app_train)\n","missing_values.head(15)"]},{"cell_type":"markdown","metadata":{},"source":["We must use the proper approach to fill in missing information, or imputation, in order to develop our machine learning model. Eliminating columns with significant missing value percentages (e.g., 40%) is another way. However, this is a loss of data, and we are unsure as to whether or not these columns will be useful as we create our model. Thus, they will remain with us. Other methods, such as XGBoost or Random Forest, do not require missing value imputation."]},{"cell_type":"markdown","metadata":{},"source":["## Checking datatypes of columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Number of each type of column\n","app_train.dtypes.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Number of unique classes in each object column\n","app_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)"]},{"cell_type":"markdown","metadata":{},"source":["Due to the presence of categorical variables with a few unique entries, it's essential to convert them into numerical format for machine learning compatibility, as most models require numerical input.\n","\n","**Encoding Strategy:**\n","- **Label Encoding** is used for binary categories, assigning an integer to each category. It's straightforward but imposes an arbitrary order.\n","- **One-Hot Encoding** is applied to variables with more than two categories, creating a new column for each category and avoiding arbitrary numerical assignments.\n","\n","For binary categories, Label Encoding is sufficient. However, for variables with more than two categories, One-Hot Encoding is preferred to prevent arbitrary value assignments. One potential drawback of One-Hot Encoding is the significant increase in dataset dimensions, which can be managed through dimensionality reduction techniques like PCA.\n","\n","In practice, we apply Label Encoding to variables with two categories and One-Hot Encoding to those with more. After encoding:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create a label encoder object\n","le = LabelEncoder()\n","le_count = 0\n","\n","# Iterate through the columns\n","for col in app_train:\n","    if app_train[col].dtype == 'object':\n","        # If 2 or fewer unique categories\n","        if len(list(app_train[col].unique())) <= 2:\n","            # Train on the training data\n","            le.fit(app_train[col])\n","            # Transform both training and testing data\n","            app_train[col] = le.transform(app_train[col])\n","            app_test[col] = le.transform(app_test[col])\n","            \n","            # Keep track of how many columns were label encoded\n","            le_count += 1\n","            \n","print('%d columns were label encoded.' % le_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# one-hot encoding of categorical variables\n","app_train = pd.get_dummies(app_train)\n","app_test = pd.get_dummies(app_test)\n","\n","print('Training Features shape: ', app_train.shape)\n","print('Testing Features shape: ', app_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["This approach ensures our dataset is fully numerical, with specific encoding strategies tailored to the number of categories in each variable."]},{"cell_type":"markdown","metadata":{},"source":["**Synchronizing Training and Testing Data**\n","\n","For consistent model training and evaluation, it's crucial that the training and testing datasets have identical features. Post one-hot encoding, the training dataset might end up with additional columns due to categories absent in the testing set. To rectify this and ensure uniformity, the datasets must be aligned.\n","\n","The alignment process involves:\n","\n","1. Preserving the `TARGET` variable from the training set as it's not present in the testing dataset but is necessary for training.\n","2. Aligning the datasets to match their columns. This step involves removing any columns from the training set that aren't found in the testing set, ensuring both datasets have the same features.\n","\n","Here's how to implement these steps:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_labels = app_train['TARGET']\n","\n","# Align the training and testing data, keep only columns present in both dataframes\n","app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n","\n","# Add the target back in\n","app_train['TARGET'] = train_labels\n","\n","print('Training Features shape: ', app_train.shape)\n","print('Testing Features shape: ', app_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Post-alignment, the datasets are now harmonized, containing an identical set of features which is essential for the integrity of model training and evaluation processes."]},{"cell_type":"markdown","metadata":{},"source":["\n","**Detecting Data Anomalies**\n","\n","Identify anomalies by examining column statistics, particularly for columns like `DAYS_BIRTH`, which are recorded relative to the loan application date and are negative. Converting `DAYS_BIRTH` to positive and representing it in years:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for col in app_train:\n","    print(col, app_train[col].dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["(app_train['DAYS_BIRTH'] / -365).describe()"]},{"cell_type":"markdown","metadata":{},"source":["Those ages look reasonable viz. (20, 70). There are no outliers for the age on either the high or low end. How about the days of employment?"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["app_train['DAYS_EMPLOYED'].describe()"]},{"cell_type":"markdown","metadata":{},"source":["That doesn't look right! The maximum value (besides being positive) is about 1000 years. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\n","plt.xlabel('Days Employment')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Just out of curiousity, let's subset the anomalous clients and see if they tend to have higher or low rates of default than the rest of the clients."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\n","non_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\n","print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\n","print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\n","print('There are %d anomalous values in column days of employment' % len(anom))"]},{"cell_type":"markdown","metadata":{},"source":["It turns out that the anomalies have a lower rate of default.\n","\n","Handling the anomalies depends on the exact situation, with no set rules. One of the safest approaches is just to set the anomalies to a missing value and then have them filled in (using Imputation) before machine learning. In this case, since all the anomalies have the exact same value, we want to fill them in with the same value in case all of these loans share something in common. The anomalous values seem to have some importance, so we want to tell the machine learning model if we did in fact fill in these values. As a solution, we will fill in the anomalous values with not a number (np.nan) and then create a new boolean column indicating whether or not the value was anomalous."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create an anomalous flag column\n","app_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n","\n","# Replace the anomalous values with nan\n","app_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n","\n","app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram')\n","plt.xlabel('Days Employment')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The distribution now appears more realistic and aligned with expectations. Additionally, a new column has been introduced to inform the model about the original anomalous values. This is crucial since we'll need to impute the missing values (now represented as NaNs) with a representative statistic, likely the column's median."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Describing the distribution of the 'DAYS_REGISTRATION' feature\n","app_train['DAYS_REGISTRATION'].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Describing the distribution of the 'DAYS_ID_PUBLISH' feature\n","app_train['DAYS_ID_PUBLISH'].describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Describing the distribution of the 'OWN_CAR_AGE' feature\n","app_train['OWN_CAR_AGE'].describe()"]},{"cell_type":"markdown","metadata":{},"source":["The other columns with DAYS in the dataframe look to be about what we expect with no obvious outliers.\n","\n","As an extremely important note, anything we do to the training data we also have to do to the testing data. Let's make sure to create the new column and fill in the existing column with np.nan in the testing data."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Identifying anomalies in 'DAYS_EMPLOYED' for the test dataset\n","app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\n","# Replacing identified anomalies with NaN for consistency\n","app_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\n","# Reporting the number of anomalies found in the test data\n","print('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))\n","# Calculation to find the percentage of anomalies in the test dataset\n","percentage_anomalies = 9274 * 100 / 48744\n","print(f'Percentage of anomalies in the test data: {percentage_anomalies:.2f}%')"]},{"cell_type":"markdown","metadata":{},"source":["### Correlations\n","Exploring correlations between features and the target variable is a fundamental part of understanding the data. By calculating the Pearson correlation coefficient, we can gauge linear relationships between each variable and the target. Although the correlation coefficient might not fully capture the \"relevance\" of a feature, it provides insights into potential associations within the data. The correlation values range from -1 to 1, where values closer to 1 or -1 indicate a strong positive or negative relationship, respectively, and values near 0 suggest no linear correlation. Here's a breakdown of how to interpret the absolute value of the correlation coefficient:\n","\n","- **0.00-0.19:** Very weak\n","- **0.20-0.39:** Weak\n","- **0.40-0.59:** Moderate\n","- **0.60-0.79:** Strong\n","- **0.80-1.0:** Very strong\n","\n","To identify and display the most significant correlations with the target variable:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Calculating correlations between all features and the target\n","correlations = app_train.corr()['TARGET'].sort_values()\n","\n","# Displaying the top 15 positive correlations\n","print('Most Positive Correlations:\\n', correlations.tail(15))\n","\n","# Displaying the top 15 negative correlations\n","print('\\nMost Negative Correlations:\\n', correlations.head(15))"]},{"cell_type":"markdown","metadata":{},"source":["This process highlights the features most positively and negatively correlated with the target, guiding further analysis and feature selection for modeling.\n","\n","Exploring the significant correlations further, we find that `DAYS_BIRTH` shows the highest positive correlation. This feature represents the client's age in days (recorded as negative values) at the time of the loan application. Despite the positive correlation, the feature's negative value implies an inverse relationship: older clients are less likely to default on their loans. To clarify, converting `DAYS_BIRTH` to absolute values would reveal a negative correlation, indicating that age is inversely related to default risk. However, it's important to note that all identified correlations are very weak."]},{"cell_type":"markdown","metadata":{},"source":["### Effect of Age (ie `DAYS_BIRTH`) on repayment"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Find the correlation of the positive days since birth and target\n","app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\n","app_train['DAYS_BIRTH'].corr(app_train['TARGET'])"]},{"cell_type":"markdown","metadata":{},"source":["As the client gets older, there is a negative linear relationship with the target meaning that as clients get older, they tend to repay their loans on time more often."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Set the style of plots\n","plt.style.use('fivethirtyeight')\n","\n","# Plot the distribution of ages in years\n","plt.hist(app_train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 50)\n","plt.title('Age of Client')\n","plt.xlabel('Age (years)')\n","plt.ylabel('Count')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["To illustrate the impact of age on the target variable, we'll proceed by generating a Kernel Density Estimation (KDE) plot, where the color represents the target value.\n","\n","A KDE plot portrays the distribution of a single variable, akin to a smoothed histogram. It achieves this by calculating a kernel, often Gaussian, at each data point and subsequently averaging these kernels to form a single smooth curve.\n","\n","For this visualization, we'll utilize the seaborn kdeplot function."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize = (8, 6))\n","\n","# KDE plot of loans that were repaid on time\n","sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH'] / 365, label = 'Loans repayed on time')\n","\n","# KDE plot of loans which were not repaid on time\n","sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, 'DAYS_BIRTH'] / 365, label = 'Loans not repayed on time')\n","\n","# Labeling of plot\n","plt.xlabel('Age (years)')\n","plt.ylabel('Density')\n","plt.title('Distribution of Ages')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The curve representing loans not repaid on time tends to skew towards younger individuals within the age range. Although this correlation may not be significant, it's likely to be valuable in machine learning models as it impacts the target variable. Let's explore this relationship from a different perspective: the average loan repayment failure rate across age groups.\n","\n","To visualize this relationship, we'll categorize age into bins of 5-year intervals. Then, we'll calculate the average target value for each age bin, indicating the proportion of loans not repaid in each age category."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Age information into a separate dataframe\n","age_data = app_train[['TARGET', 'DAYS_BIRTH']]\n","age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365\n","\n","# Bin the age data\n","age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))\n","age_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Group by the bin and calculate averages\n","age_groups  = age_data.groupby('YEARS_BINNED').mean()\n","age_groups"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Graph the age bins and the average of the target as a bar plot\n","plt.bar(age_groups.index.astype(str), 100 * age_groups['TARGET'])\n","\n","# Plot labeling\n","plt.xticks(rotation = 75)\n","plt.xlabel('Age Group (years)')\n","plt.ylabel('Failure to Repay (%)')\n","plt.title('Failure to Repay by Age Group')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["There is a clear trend: younger applicants are more likely to not repay the loan! The rate of failure to repay is above 10% for the youngest three age groups and below 5% for the oldest age group.\n","\n","This is information that could be directly used by the bank: because younger clients are less likely to repay the loan, maybe they should be provided with more guidance or financial planning tips. This does not mean the bank should discriminate against younger clients, but it would be smart to take precautionary measures to help younger clients pay on time."]},{"cell_type":"markdown","metadata":{},"source":["### External Data Sources viz. `EXT_SOURCE_1`, `EXT_SOURCE_2`, `EXT_SOURCE_3`\n","The 3 variables with the strongest negative correlations with the target are `EXT_SOURCE_1`, `EXT_SOURCE_2`, and `EXT_SOURCE_3`. According to the documentation, these features represent a \"normalized score from external data source\".\n","\n","Lets take a look at these features-"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Extract the EXT_SOURCE variables and show correlations\n","ext_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n","ext_data_corrs = ext_data.corr()\n","ext_data_corrs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Displaying a heatmap of correlations with a different color map\n","sns.heatmap(ext_data_corrs, cmap='viridis', vmin=-0.25, annot=True, vmax=0.6)\n","plt.title('Correlation Heatmap')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["The three EXT_SOURCE features exhibit negative correlations with the TARGET variable, suggesting that higher values of EXT_SOURCE are associated with a higher likelihood of loan repayment by the client. Additionally, there is a positive correlation between DAYS_BIRTH and EXT_SOURCE_1, implying that client age might be a contributing factor to this score."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize = (8, 12))\n","\n","# iterate through the sources\n","for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n","    \n","    # create a new subplot for each source\n","    plt.subplot(3, 1, i + 1)\n","    # plot repaid loans\n","    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'Loans repayed on time')\n","    # plot loans that were not repaid\n","    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'Loans not repayed on time')\n","    \n","    # Label the plots\n","    plt.title('Distribution of %s by TARGET Value' % source)\n","    plt.xlabel('%s' % source)\n","    plt.ylabel('Density')\n","    plt.legend()\n","    \n","plt.tight_layout(h_pad = 2.5)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["\"EXT_SOURCE_3 demonstrates the most noticeable variance concerning the target values, indicating a certain association with loan repayment likelihood. Although the correlation is not particularly strong, these variables still hold predictive value for machine learning models assessing loan repayment.\n","\n","For a final exploratory analysis, we'll generate a pairs plot involving the EXT_SOURCE variables and DAYS_BIRTH. This plot provides insights into relationships among multiple variable pairs and distributions of individual variables. Utilizing the seaborn visualization library and PairGrid function, the pairs plot comprises scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots with correlation coefficients on the lower triangle.\"\n","\n","As a final exploratory plot, we can make a pairs plot of the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable. \n","\n","The `Pairs Plot` is a great exploration tool because it lets us see relationships between multiple pairs of variables as well as distributions of single variables. \n","Here we are using the seaborn visualization library and the PairGrid function to create a Pairs Plot with scatterplots on the upper triangle, histograms on the diagonal, and 2D kernel density plots and correlation coefficients on the lower triangle."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Copy the data for plotting\n","plot_data = ext_data.drop(columns = ['DAYS_BIRTH']).copy()\n","\n","# Add in the age of the client in years\n","plot_data['YEARS_BIRTH'] = age_data['YEARS_BIRTH']\n","\n","# Drop na values and limit to first 100000 rows as sample\n","plot_data = plot_data.dropna().loc[:100000, :]\n","\n","# Function to calculate correlation coefficient between two columns\n","def corr_func(x, y, **kwargs):\n","    r = np.corrcoef(x, y)[0][1]\n","    ax = plt.gca()\n","    ax.annotate(\"r = {:.2f}\".format(r),\n","                xy=(.2, .8), xycoords=ax.transAxes,\n","                size = 10)\n","\n","# Create the pairgrid object\n","grid = sns.PairGrid(data = plot_data, diag_sharey=False,\n","                    hue = 'TARGET', \n","                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n","\n","# Upper is a scatter plot\n","grid.map_upper(plt.scatter, alpha = 0.2)\n","\n","# Diagonal is a histogram\n","grid.map_diag(sns.kdeplot)\n","\n","# Bottom is density plot\n","grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r)\n","\n","plt.legend()\n","plt.suptitle('Ext Source and Age Features Pairs Plot', size = 20, y = 1.05)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["In this plot, the red indicates loans that were not repaid and the blue are loans that are paid. We can see the different relationships within the data. There does appear to be a moderate positive linear relationship between the `EXT_SOURCE_1` and the `DAYS_BIRTH` (or equivalently `YEARS_BIRTH`), indicating that this feature may take into account the age of the client."]},{"cell_type":"markdown","metadata":{},"source":["### Feature Engineering Introduction:\n","\n","Feature engineering encompasses the process of feature construction, where new features are generated from existing data, and feature selection, which involves selecting the most relevant features or reducing dimensionality using various techniques. We'll employ both methods to enhance our dataset."]},{"cell_type":"markdown","metadata":{},"source":["### Polynomial Features\n","In this method, we make features that are powers of existing features as well as interaction terms between existing features. These features that are a combination of multiple individual variables are called `interaction terms` because they capture the interactions between variables. In other words, while two variables by themselves may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target. Interaction terms are commonly used in statistical models to capture the effects of multiple variables.\n","\n","In the following code, we create polynomial features using the `EXT_SOURCE` variables and the `DAYS_BIRTH` variable. `Scikit-Learn` has a useful class called `PolynomialFeatures` that creates the polynomials and the interaction terms up to a specified degree. (**Note**: High degree can lead to overfitting data)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Make a new dataframe for polynomial features\n","poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n","poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n","\n","# imputer for handling missing values\n","from sklearn.impute import SimpleImputer\n","imputer = SimpleImputer(strategy = 'median')\n","\n","poly_target = poly_features['TARGET']\n","\n","poly_features = poly_features.drop(columns = ['TARGET'])\n","\n","# Need to impute missing values\n","poly_features = imputer.fit_transform(poly_features)\n","poly_features_test = imputer.transform(poly_features_test)\n","\n","from sklearn.preprocessing import PolynomialFeatures\n","                                  \n","# Create the polynomial object with specified degree\n","poly_transformer = PolynomialFeatures(degree = 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train the polynomial features\n","poly_transformer.fit(poly_features)\n","\n","# Transform the features\n","poly_features = poly_transformer.transform(poly_features)\n","poly_features_test = poly_transformer.transform(poly_features_test)\n","print('Polynomial Features shape: ', poly_features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["poly_transformer.get_feature_names_out(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])"]},{"cell_type":"markdown","metadata":{},"source":["Now, we can see whether any of these new features are correlated with the `TARGET`."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create a dataframe of the features \n","poly_features = pd.DataFrame(poly_features, \n","                             columns = poly_transformer.get_feature_names_out(['EXT_SOURCE_1', 'EXT_SOURCE_2',\n","                                                                               'EXT_SOURCE_3', 'DAYS_BIRTH']))\n","\n","# Add in the target\n","poly_features['TARGET'] = poly_target\n","\n","# Find the correlations with the target\n","poly_corrs = poly_features.corr()['TARGET'].sort_values(ascending=False)\n","# Display most negative and most positive correlations\n","print('Most Positive Correlations:\\n', poly_corrs.head(10))\n","print('\\nMost Negative Correlations:\\n', poly_corrs.tail(5))"]},{"cell_type":"markdown","metadata":{},"source":["Several of the new variables have a greater (in terms of absolute magnitude) correlation with the target than the original features. When we build machine learning models, we can try with and without these features to determine if they actually help the model learn.\n","\n","We will add these features to a copy of the training and testing data and then evaluate models with and without the features. Many times in machine learning, the only way to know if an approach will work is to try it out!"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Put test features into dataframe\n","poly_features_test = pd.DataFrame(poly_features_test, \n","                                  columns = poly_transformer.get_feature_names_out(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n","                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n","\n","# Merge polynomial features into training dataframe\n","poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n","app_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n","\n","# Merge polnomial features into testing dataframe\n","poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n","app_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n","\n","# Align the dataframes\n","app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n","\n","# Print out the new shapes\n","print('Training data with polynomial features shape: ', app_train_poly.shape)\n","print('Testing data with polynomial features shape:  ', app_test_poly.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Domain Knowledge features\n","We can make a couple features that attempt to capture what we think may be important for telling whether a client will default on a loan. In this case we can create some features using our knowledge in  finance domain.\n","\n","These are some of the examples of these features:\n","\n","    1. CREDIT_INCOME_PERCENT: the percentage of the credit amount relative to a client's income\n","    2. ANNUITY_INCOME_PERCENT: the percentage of the loan annuity relative to a client's income\n","    3. CREDIT_TERM: the length of the payment in months (since the annuity is the monthly amount due\n","    4. DAYS_EMPLOYED_PERCENT: the percentage of the days employed relative to the client's age\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["app_train_domain = app_train.copy()\n","app_test_domain = app_test.copy()\n","\n","app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n","app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n","app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n","app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n","app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n","app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n","app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']"]},{"cell_type":"markdown","metadata":{},"source":["**Visualize New Variables**\n","\n","We should explore these domain knowledge variables visually in a graph. For all of these, we will make the same KDE plot colored by the value of the `TARGET`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize = (8, 12))\n","\n","# iterate through the features\n","for i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n","    \n","    # create a new subplot for each feature\n","    plt.subplot(4, 1, i + 1)\n","    # plot repaid loans\n","    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'Loans repayed on time')\n","    # plot loans that were not repaid\n","    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'Loans not repayed on time')\n","    \n","    # Label the plots\n","    plt.title('Distribution of %s by TARGET Value' % feature)\n","    plt.xlabel('%s' % feature)\n","    plt.ylabel('Density')\n","    plt.legend()\n","    \n","plt.tight_layout(h_pad = 2.5)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["It's hard to say ahead of time if these new features will be useful."]},{"cell_type":"markdown","metadata":{},"source":["# Model "]},{"cell_type":"markdown","metadata":{},"source":["For a naive baseline, we could guess the same value for all examples on the testing set. We are asked to predict the probability of not repaying the loan, so if we are entirely unsure, we would guess 0.5 for all observations on the test set. This will get us a Reciever Operating Characteristic Area Under the Curve (AUC ROC) of 0.5 in the competition (random guessing on a classification task will score a 0.5).\n","\n","Since we already know what score we are going to get, we don't really need to make a naive baseline guess. \n","\n","Let's use a slightly more sophisticated model for our actual baseline: Logistic Regression."]},{"cell_type":"markdown","metadata":{},"source":["# Logistic Regression Implementation"]},{"cell_type":"markdown","metadata":{},"source":["We will use `LogisticRegression` from `Scikit-Learn` for our first model. The only change we will make from the default model settings is to lower the regularization parameter, `C`, which controls the amount of overfitting (a lower value should decrease overfitting). This will get us slightly better results than the default LogisticRegression, but it still will set a low bar for any future models."]},{"cell_type":"markdown","metadata":{},"source":["To get a baseline, we will use all of the features after encoding the categorical variables. \n","\n","We will preprocess the data by filling in the missing values (imputation) and normalizing the range of the features (feature scaling). "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score\n","\n","# Assuming app_train and app_test are your training and testing datasets, respectively\n","\n","# Drop the target from the training data\n","if 'TARGET' in app_train:\n","    X = app_train.drop(columns=['TARGET'])\n","    y = app_train['TARGET']\n","else:\n","    X = app_train.copy()\n","    y = None  # Adjust this based on your data\n","\n","# Copy of the testing data\n","test = app_test.copy()\n","\n","# Median imputation of missing values\n","imputer = SimpleImputer(strategy='median')\n","\n","# Scale each feature to 0-1\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","# Impute missing values in the training and test sets\n","X_imputed = imputer.fit_transform(X)\n","test_imputed = imputer.transform(test)\n","\n","# Scale the data\n","X_scaled = scaler.fit_transform(X_imputed)\n","test_scaled = scaler.transform(test_imputed)\n","\n","# Split the training data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Initialize and fit the logistic regression model\n","log_reg = LogisticRegression(max_iter=1000, solver='liblinear')\n","log_reg.fit(X_train, y_train)\n","\n","# Predict probabilities for training and validation sets\n","train_proba = log_reg.predict_proba(X_train)[:, 1]\n","val_proba = log_reg.predict_proba(X_val)[:, 1]\n","\n","# Calculate AUC scores for training and validation sets\n","train_auc = roc_auc_score(y_train, train_proba)\n","val_auc = roc_auc_score(y_val, val_proba)\n","\n","# Print AUC scores\n","print(f'Training AUC: {train_auc:.3f}')\n","print(f'Validation AUC: {val_auc:.3f}')\n","\n","# Predict probabilities for the test set (if needed)\n","test_proba = log_reg.predict_proba(test_scaled)[:, 1]\n","\n","submission = pd.DataFrame(app_test['SK_ID_CURR'], columns=['SK_ID_CURR'])\n","submission['TARGET'] = test_proba\n","\n","print('Submission data shape: ', submission.shape)\n","submission.head()\n","\n","# Save the submission DataFrame to a CSV file\n","submission.to_csv('logistic_regression_baseline.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","# Make the model with the specified regularization parameter\n","log_reg = LogisticRegression(C = 0.0001)\n","\n","# Train on the training data\n","log_reg.fit(train, train_labels)"]},{"cell_type":"markdown","metadata":{},"source":["Now that the model has been trained, we can use it to make predictions. We want to predict the probabilities of not paying a loan, so we use the model predict.proba method. This returns an m x 2 array where m is the number of observations. The first column is the probability of the target being 0 and the second column is the probability of the target being 1 (so for a single row, the two columns must sum to 1). We want the probability the loan is not repaid, so we will select the second column."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Make predictions\n","# Make sure to select the second column only\n","log_reg_pred = log_reg.predict_proba(test)[:, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","# Predicting probabilities for the training set for evaluation\n","train_proba = log_reg.predict_proba(train)[:, 1]\n","\n","# Calculating AUC scores\n","train_auc = roc_auc_score(train_labels, train_proba)\n","\n","# Printing AUC scores\n","print(f'Training AUC: {train_auc}')\n","\n","fpr, tpr, thresholds = roc_curve(train_labels, train_proba)\n","\n","# Plotting the ROC curve\n","plt.figure(figsize=(8, 6))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % train_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Submission dataframe\n","submit = app_test[['SK_ID_CURR']]\n","submit['TARGET'] = log_reg_pred\n","\n","submit.head()"]},{"cell_type":"markdown","metadata":{},"source":["The predictions represent a probability between 0 and 1 that the loan will not be repaid. If we were using these predictions to classify applicants, we could set a probability threshold for determining that a loan is risky.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save the submission DataFrame to a CSV file\n","submit.to_csv('logistic_regression_baseline.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["The submission has now been saved to the virtual environment in which our notebook is running. To access the submission, at the end of the notebook, we will hit the blue Commit & Run button at the upper right of the kernel. This runs the entire notebook and then lets us download any files that are created during the run.\n","\n","Once we run the notebook, the files created are available in the Versions tab under the Output sub-tab. From here, the submission files can be submitted to the competition or downloaded. Since there are several models in this notebook, there will be multiple output files."]},{"cell_type":"markdown","metadata":{},"source":["**Score**: 0.690"]},{"cell_type":"markdown","metadata":{},"source":["# Random Forest Implementation"]},{"cell_type":"markdown","metadata":{},"source":["Let's try using a Random Forest on the same training data to see how that affects performance. The Random Forest is a much more powerful model especially when we use hundreds of trees. We will use 100 trees in the random forest."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_auc_score\n","\n","# Assuming app_train and app_test are your DataFrame variables for training and testing datasets, respectively\n","\n","# Drop the target from the training data\n","if 'TARGET' in app_train:\n","    X = app_train.drop(columns=['TARGET'])\n","    y = app_train['TARGET']\n","else:\n","    X = app_train.copy()\n","    y = None  # Adjust based on your actual setup\n","\n","# Copy of the testing data\n","test = app_test.copy()\n","\n","# Median imputation of missing values\n","imputer = SimpleImputer(strategy='median')\n","\n","# Scale each feature to 0-1\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","# Impute missing values in the training and test sets\n","X_imputed = imputer.fit_transform(X)\n","test_imputed = imputer.transform(test)\n","\n","# Scale the data\n","X_scaled = scaler.fit_transform(X_imputed)\n","test_scaled = scaler.transform(test_imputed)\n","\n","# Split the training data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n","\n","# Initialize and fit the Random Forest classifier\n","random_forest = RandomForestClassifier(n_estimators=100, random_state=50, verbose=1, n_jobs=-1)\n","random_forest.fit(X_train, y_train)\n","\n","# Predict probabilities for training and validation sets\n","train_proba = random_forest.predict_proba(X_train)[:, 1]\n","val_proba = random_forest.predict_proba(X_val)[:, 1]\n","\n","# Calculate AUC scores for training and validation sets\n","train_auc = roc_auc_score(y_train, train_proba)\n","val_auc = roc_auc_score(y_val, val_proba)\n","\n","# Print AUC scores\n","print(f'Training AUC: {train_auc:.3f}')\n","print(f'Validation AUC: {val_auc:.3f}')\n","\n","# Predict probabilities for the test set (if needed)\n","test_proba_rf = random_forest.predict_proba(test_scaled)[:, 1]\n","\n","# Create a submission DataFrame or use the test probabilities as needed\n","# Replace 'SK_ID_CURR' with your actual ID column in the test dataset\n","submission = pd.DataFrame(app_test['SK_ID_CURR'], columns=['SK_ID_CURR'])\n","submission['TARGET'] = test_proba_rf\n","\n","print('Submission data shape: ', submission.shape)\n","submission.head()\n","\n","# Save the submission dataframe\n","submission.to_csv('random_forest_baseline.csv', index = False)\n","print(submission.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Make the random forest classifier\n","random_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train on the training data\n","random_forest.fit(train, train_labels)\n","\n","# Extract feature importances\n","feature_importance_values = random_forest.feature_importances_\n","feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n","\n","# Make predictions on the test data\n","predictions = random_forest.predict_proba(test)[:, 1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Make a submission dataframe\n","submit = app_test[['SK_ID_CURR']]\n","submit['TARGET'] = predictions\n","\n","# Save the submission dataframe\n","submit.to_csv('random_forest_baseline.csv', index = False)\n","print(submit.head())"]},{"cell_type":"markdown","metadata":{},"source":["**Score**: 0.67877"]},{"cell_type":"markdown","metadata":{},"source":["\n","### Make Predictions using Engineered Features\n","\n","The only way to see if the Polynomial Features and Domain knowledge improved the model is to train a test a model on these features! We can then compare the submission performance to that for the model without these features to gauge the effect of our feature engineering.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["poly_features_names = list(app_train_poly.columns)\n","\n","# Impute the polynomial features\n","imputer = SimpleImputer(strategy = 'median')\n","\n","poly_features = imputer.fit_transform(app_train_poly)\n","poly_features_test = imputer.transform(app_test_poly)\n","\n","# Scale the polynomial features\n","scaler = MinMaxScaler(feature_range = (0, 1))\n","\n","poly_features = scaler.fit_transform(poly_features)\n","poly_features_test = scaler.transform(poly_features_test)\n","\n","random_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train on the training data\n","random_forest_poly.fit(poly_features, train_labels)\n","\n","# Make predictions on the test data\n","predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Make a submission dataframe\n","submit = app_test[['SK_ID_CURR']]\n","submit['TARGET'] = predictions\n","\n","print(submit.head())\n","# Save the submission dataframe\n","submit.to_csv('random_forest_baseline_feature_engineered.csv', index = False)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Score**: 0.60467\n","\n","Given these results, it does not appear that our feature construction helped in this case."]},{"cell_type":"markdown","metadata":{},"source":["# Testing Domain Features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["app_train_domain = app_train_domain.drop(columns = 'TARGET')\n","\n","domain_features_names = list(app_train_domain.columns)\n","\n","# Impute the domainnomial features\n","imputer = SimpleImputer(strategy = 'median')\n","\n","domain_features = imputer.fit_transform(app_train_domain)\n","domain_features_test = imputer.transform(app_test_domain)\n","\n","# Scale the domainnomial features\n","scaler = MinMaxScaler(feature_range = (0, 1))\n","\n","domain_features = scaler.fit_transform(domain_features)\n","domain_features_test = scaler.transform(domain_features_test)\n","\n","random_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n","\n","# Train on the training data\n","random_forest_domain.fit(domain_features, train_labels)\n","\n","# Extract feature importances\n","feature_importance_values_domain = random_forest_domain.feature_importances_\n","feature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n","\n","# Make predictions on the test data\n","predictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Make a submission dataframe\n","submit = app_test[['SK_ID_CURR']]\n","submit['TARGET'] = predictions\n","\n","# Save the submission dataframe\n","submit.to_csv('random_forest_baseline_feature_domain.csv', index = False)"]},{"cell_type":"markdown","metadata":{},"source":["**Score**: 0.67996\n","\n","Given these results, it does not appear that our feature construction helped much in this case."]},{"cell_type":"markdown","metadata":{},"source":["# Model Interpretation: Feature Importances"]},{"cell_type":"markdown","metadata":{},"source":["\n","As a simple method to see which variables are the most relevant, we can look at the feature importances of the random forest. Given the correlations we saw in the exploratory data analysis, we should expect that the most important features are the `EXT_SOURCE` and the `DAYS_BIRTH`. We may use these feature importances as a method of dimensionality reduction in future work.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_feature_importances(df):\n","    \"\"\"\n","    Plot importances returned by a model. This can work with any measure of\n","    feature importance provided that higher importance is better. \n","    \n","    Args:\n","        df (dataframe): feature importances. Must have the features in a column\n","        called `features` and the importances in a column called `importance`\n","        \n","    Returns:\n","        shows a plot of the 15 most importance features\n","        \n","        df (dataframe): feature importances sorted by importance (highest to lowest) \n","        with a column for normalized importance\n","        \"\"\"\n","    \n","    # Sort features according to importance\n","    df = df.sort_values('importance', ascending = False).reset_index()\n","    \n","    # Normalize the feature importances to add up to one\n","    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n","\n","    # Make a horizontal bar chart of feature importances\n","    plt.figure(figsize = (10, 6))\n","    ax = plt.subplot()\n","    \n","    # Need to reverse the index to plot most important on top\n","    ax.barh(list(reversed(list(df.index[:15]))), \n","            df['importance_normalized'].head(15), \n","            align = 'center', edgecolor = 'k')\n","    \n","    # Set the yticks and labels\n","    ax.set_yticks(list(reversed(list(df.index[:15]))))\n","    ax.set_yticklabels(df['feature'].head(15))\n","    \n","    # Plot labeling\n","    plt.xlabel('Normalized Importance')\n","    plt.title('Feature Importances')\n","    plt.show()\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Show the feature importances for the default features\n","feature_importances_sorted = plot_feature_importances(feature_importances)"]},{"cell_type":"markdown","metadata":{},"source":["We see that there are only a handful of features with a significant importance to the model, which suggests we may be able to drop many of the features without a decrease in performance (and we may even see an increase in performance.) Feature importances are not the most sophisticated method to interpret a model or perform dimensionality reduction, but they let us start to understand what factors our model takes into account when it makes predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)"]},{"cell_type":"markdown","metadata":{},"source":["We see that all four of our hand-engineered features made it into the top 15 most important! This should give us confidence that our domain knowledge was at least partially on track."]},{"cell_type":"markdown","metadata":{},"source":["# Light Gradient Boosting Machine"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from sklearn.metrics import roc_auc_score\n","import lightgbm as lgb\n","import gc\n","\n","def model(features, test_features, encoding = 'ohe', n_folds = 5):\n","    \n","    \"\"\"Train and test a light gradient boosting model using\n","    cross validation. \n","    \n","    Parameters\n","    --------\n","        features (pd.DataFrame): \n","            dataframe of training features to use \n","            for training a model. Must include the TARGET column.\n","        test_features (pd.DataFrame): \n","            dataframe of testing features to use\n","            for making predictions with the model. \n","        encoding (str, default = 'ohe'): \n","            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n","            n_folds (int, default = 5): number of folds to use for cross validation\n","        \n","    Return\n","    --------\n","        submission (pd.DataFrame): \n","            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n","            predicted by the model.\n","        feature_importances (pd.DataFrame): \n","            dataframe with the feature importances from the model.\n","        valid_metrics (pd.DataFrame): \n","            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n","        \n","    \"\"\"\n","    \n","    # Extract the ids\n","    train_ids = features['SK_ID_CURR']\n","    test_ids = test_features['SK_ID_CURR']\n","    \n","    # Extract the labels for training\n","    labels = features['TARGET']\n","    \n","    # Remove the ids and target\n","    features = features.drop(columns = ['SK_ID_CURR', 'TARGET'])\n","    test_features = test_features.drop(columns = ['SK_ID_CURR'])\n","    \n","    \n","    # One Hot Encoding\n","    if encoding == 'ohe':\n","        features = pd.get_dummies(features)\n","        test_features = pd.get_dummies(test_features)\n","        \n","        # Align the dataframes by the columns\n","        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n","        \n","        # No categorical indices to record\n","        cat_indices = 'auto'\n","    \n","    # Integer label encoding\n","    elif encoding == 'le':\n","        \n","        # Create a label encoder\n","        label_encoder = LabelEncoder()\n","        \n","        # List for storing categorical indices\n","        cat_indices = []\n","        \n","        # Iterate through each column\n","        for i, col in enumerate(features):\n","            if features[col].dtype == 'object':\n","                # Map the categorical features to integers\n","                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n","                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n","\n","                # Record the categorical indices\n","                cat_indices.append(i)\n","    \n","    # Catch error if label encoding scheme is not valid\n","    else:\n","        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n","        \n","    print('Training Data Shape: ', features.shape)\n","    print('Testing Data Shape: ', test_features.shape)\n","    \n","    # Extract feature names\n","    feature_names = list(features.columns)\n","    \n","    # Convert to np arrays\n","    features = np.array(features)\n","    test_features = np.array(test_features)\n","    \n","    # Create the kfold object\n","    k_fold = KFold(n_splits = n_folds, shuffle = True, random_state = 50)\n","    \n","    # Empty array for feature importances\n","    feature_importance_values = np.zeros(len(feature_names))\n","    \n","    # Empty array for test predictions\n","    test_predictions = np.zeros(test_features.shape[0])\n","    \n","    # Empty array for out of fold validation predictions\n","    out_of_fold = np.zeros(features.shape[0])\n","    \n","    # Lists for recording validation and training scores\n","    valid_scores = []\n","    train_scores = []\n","    \n","    # Iterate through each fold\n","    for train_indices, valid_indices in k_fold.split(features):\n","        \n","        # Training data for the fold\n","        train_features, train_labels = features[train_indices], labels[train_indices]\n","        # Validation data for the fold\n","        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n","        \n","        # Create the model\n","        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n","                                   class_weight = 'balanced', learning_rate = 0.05, \n","                                   reg_alpha = 0.1, reg_lambda = 0.1, \n","                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n","        \n","        # Train the model\n","        model.fit(train_features, train_labels, eval_metric = 'auc',\n","                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n","                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n","                  early_stopping_rounds = 100, verbose = 200)\n","        \n","        # Record the best iteration\n","        best_iteration = model.best_iteration_\n","        \n","        # Record the feature importances\n","        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n","        \n","        # Make predictions\n","        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n","        \n","        # Record the out of fold predictions\n","        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n","        \n","        # Record the best score\n","        valid_score = model.best_score_['valid']['auc']\n","        train_score = model.best_score_['train']['auc']\n","        \n","        valid_scores.append(valid_score)\n","        train_scores.append(train_score)\n","        \n","        # Clean up memory\n","        gc.enable()\n","        del model, train_features, valid_features\n","        gc.collect()\n","        \n","    # Make the submission dataframe\n","    submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\n","    \n","    # Make the feature importance dataframe\n","    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n","    \n","    # Overall validation score\n","    valid_auc = roc_auc_score(labels, out_of_fold)\n","    \n","    # Add the overall scores to the metrics\n","    valid_scores.append(valid_auc)\n","    train_scores.append(np.mean(train_scores))\n","    \n","    # Needed for creating dataframe of validation scores\n","    fold_names = list(range(n_folds))\n","    fold_names.append('overall')\n","    \n","    # Dataframe of validation scores\n","    metrics = pd.DataFrame({'fold': fold_names,\n","                            'train': train_scores,\n","                            'valid': valid_scores}) \n","    \n","    return submission, feature_importances, metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission, fi, metrics = model(app_train, app_test)\n","print('Baseline metrics')\n","print(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fi_sorted = plot_feature_importances(fi)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv('baseline_lgb.csv', index = False)"]},{"cell_type":"markdown","metadata":{},"source":["- The Light Gradient Boosting Machine (LGBM) model was utilized to predict loan defaults, employing a cross-validation approach for both training and evaluation.\n","- Data preparation involved:\n","  - Training data shape: 307,511 rows and 239 features.\n","  - Testing data shape: 48,744 rows and 239 features.\n","- Model performance over training iterations showed:\n","  - An increase in training AUC from approximately 0.798 to 0.828 as training progressed, indicating improved model fit over time.\n","  - Validation AUC scores remained consistent around 0.755 to 0.763, reflecting the model's generalization capability.\n","- Final evaluation metrics highlighted:\n","  - Training AUC scores varied slightly across different folds, ranging from 0.808 to 0.817, averaging at approximately 0.813.\n","  - Validation AUC scores also showed minor variations across folds, with an average score of approximately **0.759**.\n","- These results suggest that the LGBM model demonstrates a good balance between learning from the training data and generalizing to unseen data, with a strong predictive performance in identifying the likelihood of loan defaults."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["app_train_domain['TARGET'] = train_labels\n","\n","# Test the domain knolwedge features\n","submission_domain, fi_domain, metrics_domain = model(app_train_domain, app_test_domain)\n","print('Baseline with domain knowledge features metrics')\n","print(metrics_domain)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fi_sorted = plot_feature_importances(fi_domain)"]},{"cell_type":"markdown","metadata":{},"source":["The model with domain knowledge features showed:\n","\n","- Training and testing data shapes were (307511, 243) and (48744, 243), respectively.\n","- Early training rounds indicated AUC scores of about 0.804 to 0.805, with validation AUC around 0.762 to 0.770.\n","- A notable improvement in training AUC to 0.834 and validation AUC to 0.770 was observed.\n","- Final metrics across folds revealed:\n","  - Training AUC ranged from 0.807 to 0.832.\n","  - Validation AUC was between 0.763 and 0.770.\n","- Overall, the model achieved an average training AUC of 0.817 and a validation AUC of 0.766.\n","\n","These outcomes indicate that adding domain knowledge features enhances the model's predictive accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission_domain.to_csv('baseline_lgb_domain_features.csv', index = False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
